# projects/customer_churn_bank_code/customer_churn_bank_shap_v2.py
# éŠ€è¡Œå®¢æˆ¶æµå¤±é æ¸¬ - SHAP åˆ†æ V2 (èˆ‡ Train V2 å’Œ Config V2 å®Œç¾å°æ¥)

import logging
import warnings
import argparse
import sys
import os 
import re 
import joblib 
import matplotlib.pyplot as plt

# ==========================================
# 1. V2 æ¶æ§‹å°èˆª (Nav System)
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__)) # projects/code/
project_root = os.path.dirname(os.path.dirname(current_dir)) # WEB_MODEL_MAIN/

if project_root not in sys.path:
    sys.path.append(project_root)

# åŠ å…¥ç•¶å‰ç›®éŒ„ä»¥å°å…¥ Train V2 çš„é¡åˆ¥
if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    from config_v2 import config
    # â˜…â˜…â˜… æˆ°ç•¥é€£çµï¼šç›´æ¥å¼•ç”¨ Train V2 çš„ç‰¹å¾µå·¥ç¨‹é‚è¼¯ â˜…â˜…â˜…
    # é€™ä¿è­‰äº†æˆ‘å€‘è§£é‡‹çš„é‚è¼¯è·Ÿè¨“ç·´æ™‚ä¸€æ¨¡ä¸€æ¨£ï¼
    from projects.customer_churn_bank_code.customer_churn_bank_train_v2 import FeatureEngineer
except ImportError as e:
    # æœ¬åœ°èª¿è©¦ç”¨ fallback
    sys.path.append(os.path.join(project_root, 'v2.0x', 'Web_Model_Prediction-main'))
    from config_v2 import config
    try:
        from customer_churn_bank_train_v2 import FeatureEngineer
    except ImportError:
        print("âŒ ç„¡æ³•å°å…¥ Train V2 æˆ– Config V2ï¼Œè«‹æª¢æŸ¥è·¯å¾‘ã€‚")
        sys.exit(1)

# è¼‰å…¥é–‹ç™¼ç’°å¢ƒé…ç½®
APP_CONFIG = config['development']

# è¨­ç½®è­¦å‘Šå’Œæ—¥èªŒ
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - SHAP_V2 - %(levelname)s - %(message)s')
logger = logging.getLogger('ShapV2')

try:
    import numpy as np
    import pandas as pd
    import xgboost as xgb
    import shap
except ImportError as e:
    logger.error(f"ç¼ºå°‘åº«: {e}")
    sys.exit(1)

# ==========================================
# 2. SHAP åˆ†æå™¨ (V2 ç‰ˆ)
# ==========================================
class ShapAnalyzerV2:
    def __init__(self):
        self.model_path = APP_CONFIG.MODEL_BANK_PATH
        self.meta_dir = APP_CONFIG.MODEL_META_DIR
        self.model = None
        self.feature_cols = None
        self.fe_pipeline_name = None

    def load_artifacts(self) -> bool:
        """å¾ Config æŒ‡å®šçš„çµ•å°è·¯å¾‘è¼‰å…¥æ‰€æœ‰è£å‚™"""
        
        # A. è¼‰å…¥æ¨¡å‹
        try:
            self.model = joblib.load(self.model_path)
            logger.info(f"âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ: {self.model_path}")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            return False

        # B. è¼‰å…¥ç‰¹å¾µåˆ—è¡¨ (ç¢ºä¿é †åºä¸€è‡´)
        fc_path = os.path.join(self.meta_dir, 'feature_columns.joblib')
        try:
            self.feature_cols = joblib.load(fc_path)
            logger.info(f"âœ… ç‰¹å¾µåˆ—è¡¨åŠ è¼‰æˆåŠŸ ({len(self.feature_cols)} cols)")
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾µåˆ—è¡¨åŠ è¼‰å¤±æ•—: {e}")
            return False

        # C. è¼‰å…¥ FE åç¨±
        fn_path = os.path.join(self.meta_dir, 'fe_pipeline_name.txt')
        try:
            with open(fn_path, 'r') as f:
                self.fe_pipeline_name = f.read().strip()
            logger.info(f"â„¹ï¸ ä½¿ç”¨ç‰¹å¾µå·¥ç¨‹: {self.fe_pipeline_name}")
        except Exception:
            logger.warning("âš ï¸ ç„¡æ³•è®€å– FE åç¨±ï¼Œå°‡é è¨­ä½¿ç”¨ V2")
            self.fe_pipeline_name = 'run_v2_preprocessing'

        return True

    def process_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨ Train V2 çš„é‚è¼¯æ¸…æ´—æ•¸æ“š"""
        # 1. å–å¾—å°æ‡‰çš„æ¸…æ´—å‡½æ•¸
        fe_func = FeatureEngineer.FE_PIPELINES.get(self.fe_pipeline_name, FeatureEngineer.run_v2_preprocessing)
        
        # 2. æ¸…æ´— (æ³¨æ„ï¼šé€™è£¡æ˜¯è§£é‡‹éšæ®µï¼Œè¦–ç‚º is_train=True ä»¥ä¿ç•™åŸå§‹åˆ†ä½ˆï¼Œæˆ– False çœ‹éœ€æ±‚)
        # é€šå¸¸ç‚ºäº† SHAP èƒ½çœ‹åˆ°ç‰¹å¾µå…¨è²Œï¼Œæˆ‘å€‘è™•ç†æ–¹å¼èˆ‡è¨“ç·´é›†ä¸€è‡´
        df_processed = fe_func(df, is_train=True)
        
        # 3. OHE
        cat_cols = [c for c in df_processed.columns if df_processed[c].dtype.name in ['object', 'str']]
        X_oh = pd.get_dummies(df_processed, columns=cat_cols, dummy_na=False)
        
        # 4. å¼·åˆ¶å°é½Š (Alignment) - é€™æ˜¯ V2 çš„æ ¸å¿ƒé˜²ç¦¦
        missing = set(self.feature_cols) - set(X_oh.columns)
        for c in missing: X_oh[c] = 0.0
        
        # æ’åºèˆ‡ç¯©é¸
        X_final = X_oh[[c for c in self.feature_cols if c in X_oh.columns]]
        return X_final.astype(float)

    def run_shap(self, X_df, n_samples=1000):
        """åŸ·è¡Œ SHAP è¨ˆç®—ä¸¦ç•«åœ–"""
        if X_df.shape[0] > n_samples:
            X_sample = X_df.sample(n=n_samples, random_state=42)
        else:
            X_sample = X_df

        # --- é»‘é­”æ³•ï¼šä¿®å¾© XGBoost JSON base_score å•é¡Œ ---
        # é€™æ˜¯ç‚ºäº†è®“ SHAP èƒ½è®€æ‡‚ XGBoost æ¨¡å‹çš„å¿…è¦æ‰‹æ®µ
        final_model = self.model
        temp_json = os.path.join(self.meta_dir, "shap_temp.json")
        
        try:
            booster = self.model.get_booster()
            booster.save_model(temp_json)
            
            with open(temp_json, 'r') as f: content = f.read()
            # Regex ä¿®å¾© "[0.5]" -> "0.5"
            new_content = re.sub(r'"base_score":\s*"\[(.*?)\]"', r'"base_score": "\1"', content)
            
            with open(temp_json, 'w') as f: f.write(new_content)
            
            clean_booster = xgb.Booster()
            clean_booster.load_model(temp_json)
            final_model = clean_booster
            logger.info("ğŸ”§ XGBoost æ¨¡å‹å…ƒæ•¸æ“šå·²ä¿®å¾©")
            
        except Exception as e:
            logger.warning(f"ç„¡æ³•åŸ·è¡Œ XGBoost ä¿®å¾©ï¼Œå˜—è©¦ç›´æ¥ä½¿ç”¨åŸæ¨¡å‹: {e}")
        finally:
            if os.path.exists(temp_json): os.remove(temp_json)

        # --- è¨ˆç®— SHAP ---
        logger.info("ğŸ§  é–‹å§‹è¨ˆç®— SHAP å€¼...")
        explainer = shap.TreeExplainer(final_model)
        shap_values = explainer.shap_values(X_sample)

        # --- ç•«åœ–ä¸¦å­˜æª” ---
        # é€™è£¡å­˜åˆ° MODEL_META_DIRï¼Œé€™æ¨£ API å°±èƒ½è®€åˆ°äº†ï¼
        output_path = os.path.join(self.meta_dir, "shap_summary_plot.png")
        
        plt.figure()
        shap.summary_plot(shap_values, X_sample, show=False)
        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()
        
        logger.info(f"ğŸ¨ å…¨å±€ SHAP åœ–å·²ç”Ÿæˆ: {output_path}")

# ==========================================
# 3. å…¥å£
# ==========================================
def main():
    # å‡è¨­è¨“ç·´æ•¸æ“šåœ¨åŒç›®éŒ„
    data_path = os.path.join(current_dir, "customer_churn_bank_train.csv")
    if not os.path.exists(data_path):
        logger.error(f"æ‰¾ä¸åˆ°æ•¸æ“š: {data_path}")
        return

    df = pd.read_csv(data_path)
    
    analyzer = ShapAnalyzerV2()
    if not analyzer.load_artifacts():
        return

    logger.info("æ­£åœ¨è™•ç†æ•¸æ“š...")
    X_final = analyzer.process_data(df)
    
    analyzer.run_shap(X_final, n_samples=2000)
    print("\nğŸ‰ SHAP V2 åˆ†æå®Œæˆï¼API ç¾åœ¨å¯ä»¥é¡¯ç¤ºå…¨å±€è§£é‡‹åœ–äº†ã€‚")

if __name__ == "__main__":
    main()