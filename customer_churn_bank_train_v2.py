# projects/customer_churn_bank_code/customer_churn_bank_train_v2.py
# éŠ€è¡Œå®¢æˆ¶æµå¤±é æ¸¬ - V2 æ¶æ§‹æ•´åˆç‰ˆ (XGBoost Optuna/SHAP)
# ç‰¹é»ï¼šç›´æ¥è®€å– config_v2ï¼Œç¢ºä¿æ¨¡å‹ç”¢å‡ºä½ç½®èˆ‡ App è®€å–ä½ç½®å®Œå…¨ä¸€è‡´

import logging
import warnings
import argparse
import sys
import os 
from typing import Any, Callable, Tuple, Dict, List
import joblib 

# ==========================================
# 1. V2 æ¶æ§‹å°èˆªç³»çµ± (Nav System)
# ==========================================
# ç¢ºä¿èƒ½æ‰¾åˆ°æ ¹ç›®éŒ„çš„ config_v2
current_dir = os.path.dirname(os.path.abspath(__file__)) # projects/code/
project_root = os.path.dirname(os.path.dirname(current_dir)) # WEB_MODEL_MAIN/

if project_root not in sys.path:
    sys.path.append(project_root)

try:
    from config_v2 import config
except ImportError:
    # ç‚ºäº†é˜²æ­¢è·¯å¾‘å±¤ç´šæ²’å°å¥½ï¼Œåšå€‹å‚™ç”¨æ–¹æ¡ˆ
    sys.path.append(os.path.join(project_root, 'v2.0x', 'Web_Model_Prediction-main'))
    from config_v2 import config

# è¼‰å…¥é–‹ç™¼ç’°å¢ƒé…ç½® (å–å¾—è·¯å¾‘è³‡è¨Š)
APP_CONFIG = config['development']

# è¨­ç½®è­¦å‘Šå’Œæ—¥èªŒ
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - V2_TRAIN - %(levelname)s - %(message)s')
logger = logging.getLogger('TrainV2')

# æª¢æŸ¥å¿…è¦çš„åº«
try:
    import numpy as np
    import pandas as pd
    try:
        from xgboost import XGBClassifier
    except ImportError as e:
        logger.error(f"éŒ¯èª¤: ç¼ºå°‘å¿…è¦çš„åº«: {e}")
        sys.exit(1)
        
    from sklearn.model_selection import StratifiedKFold
    from sklearn.metrics import roc_auc_score
    from sklearn.base import clone
    import optuna
except ImportError as e:
    logger.error(f"éŒ¯èª¤: ç¼ºå°‘å¿…è¦çš„åº«: {e}")
    sys.exit(1)

# ==========================================
# 2. è¨“ç·´åƒæ•¸é…ç½® (ä¸å†ä½¿ç”¨æœ¬åœ° class Config)
# ==========================================
class TrainConfig:
    TARGET_COL = 'Exited'
    N_SPLITS = 5
    RANDOM_STATE = 42
    
    # â˜…â˜…â˜… é—œéµæ”¹å‹•ï¼šè·¯å¾‘ç›´æ¥å¾ APP_CONFIG è®€å– â˜…â˜…â˜…
    # é€™æ¨£è¨“ç·´å®Œçš„æ¨¡å‹ï¼ŒApp V2 ç›´æ¥å°±èƒ½ç”¨ï¼
    MODEL_PATH = APP_CONFIG.MODEL_BANK_PATH
    MODEL_META_DIR = APP_CONFIG.MODEL_META_DIR
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs(MODEL_META_DIR, exist_ok=True)

logger.info(f"ğŸ¯ ç›®æ¨™æ¨¡å‹è·¯å¾‘: {TrainConfig.MODEL_PATH}")
logger.info(f"ğŸ“‚ å…ƒæ•¸æ“šç›®éŒ„: {TrainConfig.MODEL_META_DIR}")

# --- ç‰¹å¾µå·¥ç¨‹é¡åˆ¥ (ä¿ç•™åŸæœ¬ç²¾è¯) ---
class FeatureEngineer:
    """ç”¨æ–¼ç‰¹å¾µå·¥ç¨‹çš„å·¥å…·é¡åˆ¥ã€‚"""
    @staticmethod
    def map_columns(df: pd.DataFrame, mappings: dict) -> pd.DataFrame:
        df_copy = df.copy()
        for col, mapping in mappings.items():
            if col in df_copy.columns:
                df_copy[col] = df_copy[col].map(mapping)
        return df_copy

    @staticmethod
    def cast_columns(df: pd.DataFrame, int_cols: Any = None, cat_cols: Any = None) -> pd.DataFrame:
        df_copy = df.copy()
        if int_cols:
            for col in int_cols:
                if col in df_copy.columns:
                    df_copy[col] = df_copy[col].fillna(0).astype(int) 
        return df_copy

    @staticmethod
    def run_v1_preprocessing(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:
        df_copy = df.copy()
        gender_map = {'Male': 0, 'Female': 1}
        
        df_copy = FeatureEngineer.map_columns(df_copy, {'Gender': gender_map}) 
        if 'Gender' in df_copy.columns:
            df_copy['Gender'] = df_copy['Gender'].fillna(0).astype(int)

        if 'Geography' in df_copy.columns and df_copy['Geography'].dtype.name != 'object':
             df_copy['Geography'] = df_copy['Geography'].astype(str)
        
        if 'Age' in df_copy.columns:
            df_copy['Age_bin'] = pd.cut(df_copy['Age'], bins=[0, 25, 35, 45, 60, np.inf],
                                        labels=['very_young', 'young', 'mid', 'mature', 'senior']).astype(str)
        else:
            df_copy['Age_bin'] = 'unknown'
        
        if 'NumOfProducts' in df_copy.columns:
            df_copy['Is_two_products'] = (df_copy['NumOfProducts'] == 2)
        else:
            df_copy['Is_two_products'] = 0
            
        is_germany = (df_copy['Geography'] == 'Germany') if 'Geography' in df_copy.columns else False
        
        if 'Gender' in df_copy.columns:
            df_copy['Germany_Female'] = (is_germany & (df_copy['Gender'] == 1))
        else:
            df_copy['Germany_Female'] = 0

        if 'IsActiveMember' in df_copy.columns:
            df_copy['Germany_Inactive'] = (is_germany & (df_copy['IsActiveMember'] == 0))
        else:
            df_copy['Germany_Inactive'] = 0
            
        if 'Balance' in df_copy.columns:
            df_copy['Has_Zero_Balance'] = (df_copy['Balance'] == 0)
        else:
            df_copy['Has_Zero_Balance'] = 0

        if 'Tenure' in df_copy.columns:
            df_copy['Tenure_log'] = np.log1p(df_copy['Tenure'].clip(lower=0))
        else:
            df_copy['Tenure_log'] = 0.0

        for col in ['Is_two_products', 'Germany_Female', 'Germany_Inactive', 'Has_Zero_Balance']:
            if col in df_copy.columns:
                 df_copy[col] = df_copy[col].astype(int)

        int_cols = ['HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Is_two_products', 'Has_Zero_Balance',
                    'Germany_Female', 'Germany_Inactive', 'Gender']

        df_copy = FeatureEngineer.cast_columns(df_copy, int_cols=int_cols, cat_cols=None) 

        cols_to_drop = ['id','CustomerId', 'Tenure','Surname', 'RowNumber' ] 
        if is_train and TrainConfig.TARGET_COL in df_copy.columns:
            cols_to_drop.append(TrainConfig.TARGET_COL) 

        df_copy.drop(columns=[col for col in cols_to_drop if col in df_copy.columns], inplace=True, errors='ignore')
        
        for col in df_copy.columns:
            if df_copy[col].dtype.name not in ['object', 'category', 'str']:
                 if col not in int_cols: 
                     df_copy[col] = df_copy[col].astype(float) 

        return df_copy

    @staticmethod
    def run_v2_preprocessing(df: pd.DataFrame, is_train: bool) -> pd.DataFrame:
        original_df = df.copy() 
        df_copy = FeatureEngineer.run_v1_preprocessing(original_df.copy(), is_train=is_train)

        if all(col in original_df.columns for col in ['Balance', 'IsActiveMember', 'Age']):
            df_copy['is_mature_inactive_transit'] = (
                                                    (original_df['Balance'] == 0) & 
                                                    (original_df['IsActiveMember'] == 0) & 
                                                    (original_df['Age'] > 40)).astype(int)
        else:
            df_copy['is_mature_inactive_transit'] = 0
        
        df_copy['is_mature_inactive_transit'] = df_copy['is_mature_inactive_transit'].astype(int)
        
        if TrainConfig.TARGET_COL in df_copy.columns: 
             df_copy.drop(columns=[TrainConfig.TARGET_COL], inplace=True, errors='ignore')
        
        return df_copy
    
    FE_PIPELINES: Dict[str, Callable] = {
        'run_v2_preprocessing': run_v2_preprocessing,
        'run_v1_preprocessing': run_v1_preprocessing,
    }

# --- Optuna èª¿å„ª (ä¿ç•™åŸæ¨£) ---
class HyperparameterTuner:
    @staticmethod
    def _objective(trial: optuna.Trial, X: pd.DataFrame, y: pd.Series) -> float:
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        }
        fixed_params = {
            'random_state': TrainConfig.RANDOM_STATE,
            'verbose': 0, 'eval_metric': 'logloss', 'n_jobs': -1,
            'early_stopping_rounds': 50, 'enable_categorical': False, 
        }
        full_params = {**params, **fixed_params}
        model = XGBClassifier(**full_params)
        skf = StratifiedKFold(n_splits=TrainConfig.N_SPLITS, shuffle=True, random_state=fixed_params['random_state'])
        roc_auc_scores = []

        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]
            fit_params = {'eval_set': [(X_val, y_val)], 'verbose': False}
            try:
                model.fit(X_tr, y_tr, **fit_params)
                best_iteration = model.get_booster().best_iteration
                proba_val = model.predict_proba(X_val, iteration_range=(0, best_iteration))[:, 1]
                roc_auc_scores.append(roc_auc_score(y_val, proba_val))
            except Exception:
                return 0.0
        return float(np.mean(roc_auc_scores))

    @staticmethod
    def tune(X: pd.DataFrame, y: pd.Series, n_trials: int) -> dict:
        optuna.logging.set_verbosity(logging.WARNING)
        study = optuna.create_study(direction='maximize')
        study.optimize(lambda trial: HyperparameterTuner._objective(trial, X, y), n_trials=n_trials, show_progress_bar=True)
        return study.best_params

# --- æ¨¡å‹è¨“ç·´å™¨ (V2 ä¿®æ­£ç‰ˆ) ---
class ModelTrainer:
    def __init__(self):
        self.logger = logging.getLogger('Trainer')

    def run_experiment(self, train_df, test_df, fe_pipeline, models, target_col=TrainConfig.TARGET_COL):
        self.logger.info(f"--- å•Ÿå‹• V2 å¯¦é©— (FE: {fe_pipeline.__name__}) ---")
        test_ids = test_df['id'].copy()
        y_train = train_df[target_col].astype(int)

        X_train_processed = fe_pipeline(train_df.drop(columns=[target_col], errors='ignore').copy(), is_train=True)
        X_test_processed = fe_pipeline(test_df.copy(), is_train=False)

        cat_cols_train = [col for col in X_train_processed.columns if X_train_processed[col].dtype.name in ['object', 'str']]
        cat_cols_test = [col for col in X_test_processed.columns if X_test_processed[col].dtype.name in ['object', 'str']]
        cat_cols = list(set(cat_cols_train + cat_cols_test))

        X_train_oh = pd.get_dummies(X_train_processed, columns=cat_cols, dummy_na=False)
        X_test_oh = pd.get_dummies(X_test_processed, columns=cat_cols, dummy_na=False)
        
        feature_names = X_train_oh.columns.tolist()
        missing_cols_test = set(feature_names) - set(X_test_oh.columns)
        for c in missing_cols_test: X_test_oh[c] = 0
            
        X_test_processed = X_test_oh[[col for col in feature_names if col in X_test_oh.columns]]
        X_train_processed = X_train_oh.astype(float)
        X_test_processed = X_test_processed.astype(float)
        
        self.logger.info(f"ç‰¹å¾µæ•¸é‡: {len(feature_names)}")
        
        # ç°¡å–®è¨“ç·´æœ€ä½³æ¨¡å‹ (ä¸é‡è·‘ CV ä»¥ç¯€çœæ™‚é–“ï¼Œç›´æ¥ç”¨å…¨æ•¸æ“šæˆ–æœ€å¾Œä¸€æŠ˜)
        # é€™è£¡ç°¡åŒ–æµç¨‹ï¼Œç›´æ¥å–ç¬¬ä¸€å€‹æ¨¡å‹
        best_model = None
        for name, model in models.items():
            model.set_params(enable_categorical=False)
            model.fit(X_train_processed, y_train, verbose=False)
            best_model = model # é€™è£¡ç°¡åŒ–ï¼Œç›´æ¥æ‹¿æœ€å¾Œè¨“ç·´çš„
        
        # ç”Ÿæˆé æ¸¬
        test_preds = best_model.predict_proba(X_test_processed)[:, 1]
        
        # ç”Ÿæˆæäº¤æª” (å­˜åˆ° meta dir)
        sub_path = os.path.join(TrainConfig.MODEL_META_DIR, 'submission_v2.csv')
        submission_df = pd.DataFrame({'id': test_ids, 'Exited': test_preds})
        submission_df.to_csv(sub_path, index=False)
        self.logger.info(f"æäº¤æª”å·²å­˜: {sub_path}")

        return submission_df, {}, best_model, feature_names

    def save_v2_artifacts(self, model, fe_name, feature_cols):
        """V2 å°ˆå±¬å­˜æª”ï¼šç›´æ¥å­˜åˆ° Config æŒ‡å®šçš„çµ•å°è·¯å¾‘"""
        
        # 1. ä¿å­˜æ¨¡å‹ (XGBoost é»‘é­”æ³•ä¿®æ­£)
        try:
            if isinstance(model, XGBClassifier):
                # ç°¡å–®ä¿®å¾©ï¼šç¢ºä¿ base_score æ˜¯ float (é¿é–‹ JSON hackï¼Œç›´æ¥è¨­ç½®åƒæ•¸)
                bs = model.get_params().get('base_score', 0.5)
                if isinstance(bs, str): bs = 0.5
                model.set_params(base_score=bs)
                
            joblib.dump(model, TrainConfig.MODEL_PATH)
            self.logger.info(f"âœ… æ¨¡å‹ V2 å·²éƒ¨ç½²è‡³: {TrainConfig.MODEL_PATH}")
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")

        # 2. ä¿å­˜ç‰¹å¾µåˆ—è¡¨ (Service V2 å°é½Šç”¨)
        fc_path = os.path.join(TrainConfig.MODEL_META_DIR, 'feature_columns.joblib')
        joblib.dump(feature_cols, fc_path)
        self.logger.info(f"ğŸ“ ç‰¹å¾µåˆ—è¡¨å·²åŒæ­¥: {fc_path}")

        # 3. ä¿å­˜ FE åç¨±
        fn_path = os.path.join(TrainConfig.MODEL_META_DIR, 'fe_pipeline_name.txt')
        with open(fn_path, 'w') as f:
            f.write(fe_name)

# --- ä¸»ç¨‹å¼ ---
def main(tune=False):
    # è·¯å¾‘ä¹Ÿæ˜¯å‹•æ…‹çš„ (å‡è¨­ csv åœ¨åŒç›®éŒ„)
    data_dir = os.path.dirname(os.path.abspath(__file__))
    train_file = os.path.join(data_dir, "customer_churn_bank_train.csv")
    test_file = os.path.join(data_dir, "customer_churn_bank_test.csv")
    
    if not os.path.exists(train_file):
        logger.error(f"æ‰¾ä¸åˆ°è¨“ç·´æª”: {train_file}")
        return

    df_train = pd.read_csv(train_file)
    df_test = pd.read_csv(test_file)

    trainer = ModelTrainer()
    best_fe = FeatureEngineer.run_v2_preprocessing
    
    # åƒæ•¸è¨­ç½® (ä½ çš„æœ€ä½³åƒæ•¸)
    best_params = {
        'n_estimators': 2692, 'learning_rate': 0.0578, 'max_depth': 3,
        'random_state': TrainConfig.RANDOM_STATE, 'n_jobs': -1, 'verbose': 0
    }
    
    if tune:
        logger.info("æ­£åœ¨é€²è¡Œèª¿å„ª (é€™æœƒèŠ±é»æ™‚é–“)...")
        # (èª¿å„ªé‚è¼¯çœç•¥ï¼Œç›´æ¥ç”¨æœ€ä½³åƒæ•¸ç¤ºç¯„)
    
    model = XGBClassifier(**best_params)
    
    # åŸ·è¡Œå¯¦é©—
    _, _, trained_model, feature_cols = trainer.run_experiment(
        df_train, df_test, best_fe, {'XGB_V2': model}
    )
    
    # å­˜æª” (é—œéµä¸€æ­¥ï¼)
    trainer.save_v2_artifacts(trained_model, best_fe.__name__, feature_cols)
    print("\nğŸ‰ V2 è¨“ç·´å®Œæˆï¼å‰ç·š App ç¾åœ¨å¯ä»¥ç›´æ¥é‡å•Ÿä¸¦è¼‰å…¥æ–°æ¨¡å‹äº†ï¼")

if __name__ == "__main__":
    # é è¨­ç›´æ¥åŸ·è¡Œï¼Œä¸å¸¶åƒæ•¸
    main(tune=False)